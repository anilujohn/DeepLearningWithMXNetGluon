{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gluon `Dataset`s and `DataLoader`\n",
    "\n",
    "One of the most critical steps for model training and inference is loading the data: without data you can't do Machine Learning! In this tutorial we use the Gluon API to define a [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) and use a [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader) to iterate through the dataset in mini-batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to `Dataset`s\n",
    "\n",
    "[`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) objects are used to represent collections of data, and include methods to load and parse the data (that is often stored on disk). Gluon has a number of different [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) classes for working with image data straight out-of-the-box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We'll use the [`ArrayDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=arraydataset#mxnet.gluon.data.ArrayDataset) to introduce the idea of a [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset).\n",
    "\n",
    "We first start by generating random data `X` (with 3 variables) and corresponding random labels `y` to simulate a typical supervised learning task. We generate 10 samples and we pass them all to the [`ArrayDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=arraydataset#mxnet.gluon.data.ArrayDataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "mx.random.seed(42) # Fix the seed for reproducibility\n",
    "X = mx.random.uniform(shape=(10, 3))\n",
    "y = mx.random.uniform(shape=(10, 1))\n",
    "dataset = mx.gluon.data.dataset.ArrayDataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A key feature of a [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) is the __*ability to retrieve a single sample given an index*__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Our random data and labels were generated in memory, so this [`ArrayDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=arraydataset#mxnet.gluon.data.ArrayDataset) doesn't have to load anything from disk, but the interface is the same for all [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset)s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "[0.74707687 0.37641123 0.46362457]\n",
      "<NDArray 3 @cpu(0)>, \n",
      "[0.35440788]\n",
      "<NDArray 1 @cpu(0)>)\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 4\n",
    "sample = dataset[sample_idx]\n",
    "\n",
    "assert len(sample) == 2\n",
    "assert sample[0].shape == (3, )\n",
    "assert sample[1].shape == (1, )\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "We get a tuple of a data sample and its corresponding label, which makes sense because we passed the data `X` and the labels `y` in that order when we instantiated the [`ArrayDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=arraydataset#mxnet.gluon.data.ArrayDataset). We don't usually retrieve individual samples from [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) objects though (unless we're quality checking the output samples). Instead we use a [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Introduction to `DataLoader`\n",
    "\n",
    "A [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader) is used to create mini-batches of samples from a [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset), and provides a convenient iterator interface for looping these batches. \n",
    "\n",
    "Another benefit of using [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader) is the ability to easily load data in parallel using [`multiprocessing`](https://docs.python.org/3.6/library/multiprocessing.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It's typically much more efficient to pass a mini-batch of data through a neural network than a single sample at a time, because the computation can be performed in parallel. A required parameter of [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader) is the size of the mini-batches you want to create, called `batch_size`.\n",
    "\n",
    "You can set the `num_workers` parameter to the number of CPUs avalaible on your machine for maximum performance, or limit it to a lower number to spare resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch has shape (5, 3), and y_batch has shape (5, 1)\n",
      "X_batch has shape (5, 3), and y_batch has shape (5, 1)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "CPU_COUNT = cpu_count()\n",
    "\n",
    "data_loader = mx.gluon.data.DataLoader(dataset, \n",
    "                                       batch_size=5, \n",
    "                                       num_workers=CPU_COUNT)\n",
    "\n",
    "for X_batch, y_batch in data_loader:\n",
    "    print(\"X_batch has shape {}, and y_batch has shape {}\".format(\n",
    "        X_batch.shape, \n",
    "        y_batch.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can see 2 mini-batches of data (and labels), each with 5 samples, which makes sense given we started with a dataset of 10 samples. When comparing the shape of the batches to the samples returned by the [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset), we've gained an extra dimension at the start which is sometimes called the batch axis.\n",
    "\n",
    "Our `data_loader` loop will stop when every sample of `dataset` has been returned as part of a batch. Sometimes the dataset length isn't divisible by the mini-batch size, leaving a final batch with a smaller number of samples. [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader)'s default behavior is to return this smaller mini-batch, but this can be changed by setting the `last_batch` parameter to `discard` (which ignores the last batch) or `rollover` (which starts the next epoch with the remaining samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Machine learning with `Dataset`s and `DataLoader`s\n",
    "\n",
    "You will often use a few different [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) objects in your Machine Learning project. It's essential to separate your training dataset from testing dataset, and it's also good practice to have validation dataset (a.k.a. development dataset) that can be used for optimising hyperparameters.\n",
    "\n",
    "Many of the image [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset)s accept a function (via the optional `transform` parameter) which is applied to each sample returned by the [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Using Gluon [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) objects, we define the data to be included in each of these separate datasets. Common use cases for loading data are covered already (e.g. [`mxnet.gluon.data.vision.datasets.ImageFolderDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=imagefolderdataset#mxnet.gluon.data.vision.datasets.ImageFolderDataset)), but it's simple to create your own custom [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) classes for other types of data. You can even use included [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) objects for common datasets if you want to experiment quickly; they download and parse the data for you! In this example we use the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset from Zalando Research.\n",
    "\n",
    "Transform funcations are also useful for performing data augmentation, but can also be used for more simple data type conversion and pixel value scaling as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def transform(data, label):\n",
    "    data = data.astype('float32')/255\n",
    "    return data, label\n",
    "\n",
    "train_dataset = mx.gluon.data.vision.datasets.FashionMNIST(\n",
    "    train=True, transform=transform)\n",
    "valid_dataset = mx.gluon.data.vision.datasets.FashionMNIST(\n",
    "    train=False,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'numpy.float32'>\n",
      "Label: 8\n",
      "Label description: Bag\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pylab import imshow\n",
    "\n",
    "sample_idx = 234\n",
    "sample = train_dataset[sample_idx]\n",
    "data = sample[0]\n",
    "label = sample[1]\n",
    "label_desc = {0:'T-shirt/top', 1:'Trouser', 2:'Pullover', 3:'Dress', 4:'Coat', 5:'Sandal', 6:'Shirt', 7:'Sneaker', 8:'Bag', 9:'Ankle boot'}\n",
    "\n",
    "imshow(data[:,:,0].asnumpy(), cmap='gray')\n",
    "print(\"Data type: {}\".format(data.dtype))\n",
    "print(\"Label: {}\".format(label))\n",
    "print(\"Label description: {}\".format(label_desc[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When training machine learning models it is important to shuffle the training samples every time you pass through the dataset (i.e. each epoch). Sometimes the order of your samples will have a spurious relationship with the target variable, and shuffling the samples helps remove this. With [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader) it's as simple as adding `shuffle=True`. You don't need to shuffle the validation and testing data though.\n",
    "\n",
    "If you have more complex shuffling requirements (e.g. when handling sequential data), take a look at [`mxnet.gluon.data.BatchSampler`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=batchsampler#mxnet.gluon.data.BatchSampler) and pass this to your [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data_loader = mx.gluon.data.DataLoader(\n",
    "    train_dataset, batch_size, shuffle=True, num_workers=CPU_COUNT)\n",
    "valid_data_loader = mx.gluon.data.DataLoader(\n",
    "    valid_dataset, batch_size, num_workers=CPU_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With both `DataLoader`s defined, we can now train a model to classify each image and evaluate the validation loss at each epoch. Our Fashion MNIST dataset has 10 classes including shirt, dress, sneakers, etc. We define a simple fully connected network with a softmax output and use cross entropy as our loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Blocks\n",
    "In Gluon, Each layer is a Block and these blocks can be stacked together to create a Neural Network. You can also create a CustomBlock that does not have to layered on top of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hybrid Blocks\n",
    "HybridBlocks allows to write custom layers that can be used in imperative programming as well as symbolic programming.  \n",
    "Hybridization is a process in MXNet to create symbolic graph of a forward computation. Once the graph is computed, its cached and reused for subsequent computations.\n",
    "Symbolic graphs provide faster computation speed which are often necessary for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon, autograd, ndarray\n",
    "\n",
    "def construct_net():\n",
    "    net = gluon.nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Dense(128, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dense(64, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dense(10))\n",
    "    return net\n",
    "\n",
    "# construct and initialize network.\n",
    "ctx =  mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "\n",
    "net = construct_net()\n",
    "net.hybridize()\n",
    "net.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "# define loss and trainer.\n",
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', \n",
    "                        {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 0.54, validation loss: 0.41\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # training loop (with autograd and trainer steps, etc.)\n",
    "    cumulative_train_loss = mx.nd.zeros(1, ctx=ctx)\n",
    "    training_samples = 0\n",
    "    for batch_idx, (data, label) in enumerate(train_data_loader):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784)) # 28*28=784\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_train_loss += loss.sum()\n",
    "        training_samples += data.shape[0]\n",
    "    train_loss = cumulative_train_loss.asscalar()/training_samples\n",
    "\n",
    "    # validation loop\n",
    "    cumulative_valid_loss = mx.nd.zeros(1, ctx)\n",
    "    valid_samples = 0\n",
    "    for batch_idx, (data, label) in enumerate(valid_data_loader):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784)) # 28*28=784\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        loss = criterion(output, label)\n",
    "        cumulative_valid_loss += loss.sum()\n",
    "        valid_samples += data.shape[0]\n",
    "    valid_loss = cumulative_valid_loss.asscalar()/valid_samples\n",
    "\n",
    "    print(\"Epoch {}, training loss: {:.2f}, validation loss: {:.2f}\".\n",
    "          format(epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using own data with included `Dataset`s\n",
    "\n",
    "Gluon has a number of different [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset) classes for working with your own image data straight out-of-the-box. You can get started quickly using the \n",
    "[`mxnet.gluon.data.vision.datasets.ImageFolderDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=imagefolderdataset#mxnet.gluon.data.vision.datasets.ImageFolderDataset) which loads images directly from a user-defined folder, and infers the label (i.e. class) from the folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We will run through an example for image classification, but a similar process applies for other vision tasks. If you already have your own collection of images to work with you should partition your data into training and test sets, and place all objects of the same class into seperate folders. Similar to:\n",
    "```\n",
    "    ./images/train/car/abc.jpg\n",
    "    ./images/train/car/efg.jpg\n",
    "    ./images/train/bus/hij.jpg\n",
    "    ./images/train/bus/klm.jpg\n",
    "    ./images/test/car/xyz.jpg\n",
    "    ./images/test/bus/uvw.jpg\n",
    "```\n",
    "\n",
    "You can download the Caltech 101 dataset if you don't already have images to work with for this example, but please note the download is 126MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "dataset_name = \"101_ObjectCategories\"\n",
    "archive_file = \"{}.tar.gz\".format(dataset_name)\n",
    "archive_path = os.path.join(data_folder, archive_file)\n",
    "data_url = \"https://s3.us-east-2.amazonaws.com/mxnet-public/\"\n",
    "\n",
    "if not os.path.isfile(archive_path):\n",
    "    mx.test_utils.download(\"{}{}\".format(data_url, archive_file), dirname = data_folder)\n",
    "    print('Extracting {} in {}...'.format(archive_file, data_folder))\n",
    "    tar = tarfile.open(archive_path, \"r:gz\")\n",
    "    tar.extractall(data_folder)\n",
    "    tar.close()\n",
    "    print('Data extracted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After downloading and extracting the data archive, we have two folders: `data/101_ObjectCategories` and `data/101_ObjectCategories_test`. We load the data into separate training and testing  [`ImageFolderDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=imagefolderdataset#mxnet.gluon.data.vision.datasets.ImageFolderDataset)s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "training_path = os.path.join(data_folder, dataset_name)\n",
    "testing_path = os.path.join(data_folder, \"{}_test\".\n",
    "                            format(dataset_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We instantiate the [`ImageFolderDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=imagefolderdataset#mxnet.gluon.data.vision.datasets.ImageFolderDataset)s by providing the path to the data\n",
    "\n",
    "Optionally, you can pass a `transform` parameter to these [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset)s as we've seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "the folder structure will be traversed to determine which image classes are available and which images correspond to each class. You must take care to ensure the same classes are both the training and testing datasets, otherwise the label encodings can get muddled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = mx.gluon.data.vision.datasets.ImageFolderDataset(\n",
    "    training_path)\n",
    "test_dataset = mx.gluon.data.vision.datasets.ImageFolderDataset(\n",
    "    testing_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Samples from these datasets are tuples of data and label.\n",
    "\n",
    "As with the Fashion MNIST dataset the labels will be integer encoded. You can use the `synsets` property of the [`ImageFolderDataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=imagefolderdataset#mxnet.gluon.data.vision.datasets.ImageFolderDataset)s to retrieve the original descriptions (e.g. `train_dataset.synsets[i]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Images are loaded from disk, decoded and optionally transformed when the `__getitem__(i)` method is called (equivalent to `train_dataset[i]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sample_idx = 539\n",
    "sample = train_dataset[sample_idx]\n",
    "data = sample[0]\n",
    "label = sample[1]\n",
    "\n",
    "imshow(data.asnumpy(), cmap='gray')\n",
    "print(\"Data type: {}\".format(data.dtype))\n",
    "print(\"Label: {}\".format(label))\n",
    "print(\"Label description: {}\".format(train_dataset.synsets[label]))\n",
    "assert label == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Using own data with custom `Dataset`s\n",
    "\n",
    "Sometimes you have data that doesn't quite fit the format expected by the included [`Dataset`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataset#mxnet.gluon.data.Dataset)s. You might be able to preprocess your data to fit the expected format, but it is easy to create your own dataset to do this.\n",
    "\n",
    "All you need to do is create a class that implements a `__getitem__` method, that returns a sample (i.e. a tuple of [`mx.nd.NDArray`](https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.NDArray)s).\n",
    "\n",
    "See the [Data Augmentation with Masks](http://mxnet.incubator.apache.org/tutorials/python/data_augmentation_with_masks.html) tutorial for an example of this.\n",
    "\n",
    "# Appendix: Upgrading from Module `DataIter` to Gluon `DataLoader`\n",
    "\n",
    "Before Gluon's [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader), MXNet used [`DataIter`](https://mxnet.incubator.apache.org/api/python/io/io.html?highlight=dataiter#mxnet.io.DataIter) objects for loading data for training and testing. [`DataIter`](https://mxnet.incubator.apache.org/api/python/io/io.html?highlight=dataiter#mxnet.io.DataIter) has a similar interface for iterating through data, but it isn't directly compatible with typical Gluon [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader) loops. Unlike Gluon [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader) which often returns a tuple of `(data, label)`, a [`DataIter`](https://mxnet.incubator.apache.org/api/python/io/io.html?highlight=dataiter#mxnet.io.DataIter) returns a [`DataBatch`](https://mxnet.incubator.apache.org/api/python/io/io.html?highlight=databatch#mxnet.io.DataBatch) object that has `data` and `label` properties. Switching to [`DataLoader`](https://mxnet.incubator.apache.org/api/python/gluon/data.html?highlight=dataloader#mxnet.gluon.data.DataLoader)s is highly recommended when using Gluon, but you'll need to take care of pre-processing steps such as augmentations in a `transform` function.\n",
    "\n",
    "So you can get up and running with Gluon quicker if you have already imlemented complex pre-processing steps using [`DataIter`](https://mxnet.incubator.apache.org/api/python/io/io.html?highlight=dataiter#mxnet.io.DataIter), we have provided a simple class to wrap existing [`DataIter`](https://mxnet.incubator.apache.org/api/python/io/io.html?highlight=dataiter#mxnet.io.DataIter) objects so they can be used in a typical Gluon training loop. You can use this class for `DataIter`s such as [`mxnet.image.ImageIter`](https://mxnet.incubator.apache.org/api/python/image/image.html?highlight=imageiter#mxnet.image.ImageIter) and [`mxnet.io.ImageRecordIter`](https://mxnet.incubator.apache.org/api/python/io/io.html?highlight=imagere#mxnet.io.ImageRecordIter) that have single data and label arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class DataIterLoader():\n",
    "    def __init__(self, data_iter):\n",
    "        self.data_iter = data_iter\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.data_iter.reset()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = self.data_iter.__next__()\n",
    "        assert len(batch.data) == len(batch.label) == 1\n",
    "        data = batch.data[0]\n",
    "        label = batch.label[0]\n",
    "        return data, label\n",
    "\n",
    "    def next(self):\n",
    "        return self.__next__() # for Python 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data_iter = mx.io.NDArrayIter(data=X, label=y, batch_size=5)\n",
    "data_iter_loader = DataIterLoader(data_iter)\n",
    "for X_batch, y_batch in data_iter_loader:\n",
    "    assert X_batch.shape == (5, 3)\n",
    "    assert y_batch.shape == (5, 1)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "display_name": "",
  "kernelspec": {
   "display_name": "Python [conda env:mxnet_py]",
   "language": "python",
   "name": "conda-env-mxnet_py-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "name": ""
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
